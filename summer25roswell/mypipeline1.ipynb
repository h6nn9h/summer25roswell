{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f499e79",
   "metadata": {},
   "source": [
    "Sort and remove exact matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part was copied from Google Colab, but I think everything should still work fine\n",
    "import pandas as pd\n",
    "\n",
    "truth_csv_path = '/content/drive/MyDrive/summerwowow/truthv1.csv' #<--- set this to hand labled csv\n",
    "guess_csv_path = '/content/drive/MyDrive/summerwowow/round2_infer_moonshot_llama_3.1_8B.csv'  #<--- set this to the llm results csv\n",
    "output_path = '/content/drive/MyDrive/summerwowow/fullmismatches_round2.csv'\n",
    "\n",
    "COLUMNS_TO_COMPARE = [2, 3, 4, 5]\n",
    "\n",
    "df_truth = pd.read_csv(truth_csv_path)\n",
    "df_guess = pd.read_csv(guess_csv_path)\n",
    "\n",
    "# Convert all string values in truth and guess dataframes to lowercase for case-insensitive comparison\n",
    "print(\"Lowercasing string values in truth and guess dataframes...\")\n",
    "df_truth = df_truth.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "df_guess = df_guess.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Assert that all three dataframes have the same shape for consistent indexing\n",
    "assert df_truth.shape == df_guess.shape, \"Truth and Guess CSV files must have the same shape for comparison.\"\n",
    "\n",
    "mismatches = []\n",
    "for row_idx in range(1, df_truth.shape[0]):\n",
    "    for col_idx in COLUMNS_TO_COMPARE:\n",
    "        val_truth = df_truth.iat[row_idx, col_idx]\n",
    "        val_guess = df_guess.iat[row_idx, col_idx]\n",
    "\n",
    "        if val_truth != val_guess:\n",
    "            mismatch_value_from_guess_values = df_guess_values.iat[row_idx, col_idx]\n",
    "\n",
    "            mismatches.append({\n",
    "                'row': row_idx + 1, \n",
    "                'column_idx': col_idx, \n",
    "                'truth_value': val_truth,\n",
    "                'llm_value': val_guess,\n",
    "                'mismatch_value': mismatch_value_from_guess_values # New column with value from guess_values\n",
    "            })\n",
    "\n",
    "mismatch_df = pd.DataFrame(mismatches)\n",
    "print(mismatch_df.head(10000))\n",
    "mismatch_df.to_csv(output_path, index=False)\n",
    "print(\"saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37396792",
   "metadata": {},
   "source": [
    "Prompts and setup stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_template = \"\"\"You are a domain-aware evaluator that classifies the relationship between a predicted answer and the actual (reference) answer from genomics studies data.\n",
    "\n",
    "\n",
    "Use these standardized mismatch labels:\n",
    "\n",
    "2. Terminology Difference â€“ The predicted and actual answers refer to the exact same item using a different term\n",
    "3. Generalization â€“ The predicted answer is more general than the actual answer.\n",
    "4. Too Specific â€“ The predicted answer is more specific than the actual answer.\n",
    "5. Subset of Truth â€“ The predicted answer is only part of the actual answer; doesnt include all of the actual answers. there must be multiple possible answers denoted by a \"/\" or a \";\" or \",\"\n",
    "6. Incorrect / Out of Scope / Hallucinated â€“ The predicted answer is incorrect, unrelated, or introduces a false concept.\n",
    "\n",
    "Below are some examples for this task:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "You are looking at a case with {type_name} data.\n",
    "\n",
    "Step 1: Compare the prediction and the actual answer. What are the differences?\n",
    "Step 2: Based on those differences, classify the mismatch as type 2â€“6.\n",
    "\n",
    "Predicted: {predicted}\n",
    "Actual: {actual}\n",
    "\n",
    "\n",
    "Make sure nothing else is returned, only the number of the mismatch categor, which can be numbers 2, 3, 4, 5, or 6 (not 1). DOUBLE CHECK ITS ONLY THE NUMBER OF THE MISMATCH, only numbers should be returned, no words and no explainations. the answer should be formatted like this: \"2\" or \"3\" or \"4\" or \"5\" or \"6\" ONLY with no following text\"\"\"\n",
    "\n",
    "\n",
    "few_shot_bank = {\n",
    "    '2': [\n",
    "        (\"Predicted: ovarian tumor\\nActual: ovarian cancer\", \"2: Terminology Difference â€“ 'tumor' and 'cancer' are used interchangeably here, with no change in meaning.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: snrna-seq\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: rna-seq\\nActual: scrna-seq\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: scrna-seq; vdj-seq\", \"5: Subset of Truth â€“ prediction does not include all details from truth, or is a component of truth.\"),\n",
    "        (\"Predicted: sctcr-seq\\nActual: scrna-seq; vdj-seq\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: merip-seq\\nActual: plumage\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: cite-seq\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "    ],\n",
    "    '4': [\n",
    "        (\"Predicted: ovarian carcinoma\\nActual: ovarian cancer\", \"2: Terminology Difference â€“ different terms for the same disease.\"),\n",
    "        (\"Predicted: subq. tumor\\nActual: subcutaneous tumor\", \"2: Terminology Difference â€“ abbreviation does not affect the meaning.\"),\n",
    "        (\"Predicted: cervical lymph node\\nActual: lymph node\", \"4: Too specific â€“ prediction adds body region.\"),\n",
    "        (\"Predicted: colorectal carcinoma\\nActual: colon\", \"4: Too specific â€“ prediction adds a cancer label.\"),\n",
    "        (\"Predicted: pancreatic neuroendocrine tumor\\nActual: pancreas\", \"4: Too specific â€“ prediction adds a type of cancer.\"),\n",
    "        (\"Predicted: gastric wall\\nActual: stomach\", \"4: Too specific â€“ prediction adds a body part detail.\"),\n",
    "        (\"Predicted: hepatocellular carcinoma\\nActual: liver\", \"4: Too specific â€“ prediction adds a disease name.\"),\n",
    "        (\"Predicted: lung\\nActual: lung adenocarcinoma\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "        (\"Predicted: liver\\nActual: hepatocellular carcinoma biopsy\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "        (\"Predicted: bone marrow\\nActual: spleen\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: prostate cancer\\nActual: prostate\", \"4: Too specific â€“ prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: prostate\\nActual: prostate carcinoma\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "        (\"Predicted: breast tumor\\nActual: breast\", \"4: Too specific â€“ prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: embryonic kidney\\nActual: kidney\", \"4: Too specific â€“ prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: bone marrow\\nActual: blood\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "    ],\n",
    "    '5': [\n",
    "        (\"Predicted: lung squamous cell carcinoma cells\\nActual: lung squamous cell carcinoma\", \"2: Terminology Difference â€“ adding 'cells' doesn't change the entity.\"),\n",
    "        (\"Predicted: aspc-1\\nActual: aspc1\", \"2: Terminology Difference â€“ same cell line with a different naming style.\"),\n",
    "        (\"Predicted: rat lung adenocarcinoma cell line\\nActual: rat lung adenocarcinoma cells\", \"2: Terminology Difference â€“ 'cell line' and 'cells' are interchangeable in this context.\"),\n",
    "        (\"Predicted: immortalized human dermal fibroblast line\\nActual: immortalized human dermal fibroblasts\", \"2: Terminology Difference â€“ 'line' is included but doesnâ€™t change the meaning.\"),\n",
    "        (\"Predicted: primary mouse mammary fibroblasts line\\nActual: primary mouse mammary fibroblasts\", \"2: Terminology Difference â€“ 'line' is added, but it doesnâ€™t meaningfully change the entity.\"),\n",
    "        (\"Predicted: kp lung tumor cells\\nActual: kp cells\", \"2: Terminology Difference â€“ the added descriptors donâ€™t change the identity of the cell line.\"),\n",
    "        (\"Predicted: gata3\\nActual: gata3-expressing cells\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "        (\"Predicted: hek-ter\\nActual: hek cells\", \"4: Too specific â€“ prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: h1650\\nActual: h1651\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: hoxb8\\nActual: hoxb8 cell\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "        (\"Predicted: no\\nActual: hoxb8 cell\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: d458\\nActual: d459\", \"6: Incorrect / Out of Scope / Hallucinated â€“ completely different or unclassifiable.\"),\n",
    "        (\"Predicted: myc-cap\\nActual: myc-cap cells\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "    ],\n",
    "    '3': [\n",
    "        (\"Predicted: mus musculus\\nActual: mus musculus; synthetic construct\", \"5: Subset of Truth â€“ prediction does not include all details from truth, or is a component of truth.\"),\n",
    "        (\"Predicted: homo sapiens\\nActual: synthetic construct; homo sapiens and murine mouse model\", \"3: Generalization â€“ predicted term is broader.\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "column_type_map = {\n",
    "    2: \"Sequencing type\",\n",
    "    3: \"Organism type\",\n",
    "    4: \"Tissue type\",\n",
    "    5: \"Cell line type\",\n",
    "    6: \"Disease type\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    task = str(row[1])                 #Column 2 is case type\n",
    "    predicted = str(row[2])              #Column 3 is predicted\n",
    "    actual = str(row[3])                     #Column 4 is actual\n",
    "    column_idx = int(row['column_idx'])\n",
    "\n",
    "    type_name = column_type_map.get(column_idx, \"feature\")\n",
    "\n",
    "    # Format examples\n",
    "    examples = few_shot_bank[task]\n",
    "    few_shot_str = \"\\n\\n\".join([\n",
    "        f\"{ex[0]}\\n{ex[1]}\" for ex in examples\n",
    "    ])\n",
    "\n",
    "    return base_prompt_template.format(\n",
    "        few_shot_examples=few_shot_str,\n",
    "        predicted=predicted,\n",
    "        actual=actual,\n",
    "        type_name=type_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88fc78",
   "metadata": {},
   "source": [
    "Self-consistency + few shot + re-evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Config ===\n",
    "INPUT_CSV = \"/Users/yjyou/Documents/summer25/updated_mismatch_with_errors3.csv\"  #<---- csv generated from the first part\n",
    "OUTPUT_CSV = \"/Users/yjyou/Documents/summer25/newest_round3.csv\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"gemma3:27b\"\n",
    "SC_SAMPLES = 5\n",
    "SC_TEMPERATURE = 0.95\n",
    "ON_FAIL_FALLBACK = \"6\"\n",
    "\n",
    "# === Label Extractor ===\n",
    "digit_re = re.compile(r\"\\b([2-6])\\b\")\n",
    "def extract_label(text):\n",
    "    if not isinstance(text, str): return ON_FAIL_FALLBACK\n",
    "    match = digit_re.search(text.strip())\n",
    "    return match.group(1) if match else ON_FAIL_FALLBACK\n",
    "\n",
    "mismatch_df = pd.read_csv(INPUT_CSV)\n",
    "results = []\n",
    "start_idx = 0\n",
    "end_idx = len(mismatch_df)  # or 176 if you're limiting\n",
    "print(f\"ðŸ” Processing rows {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "for i, row in mismatch_df.iloc[start_idx:end_idx].iterrows():\n",
    "    prompt = build_prompt(row)  # Assumes you defined this already\n",
    "    if prompt is None:\n",
    "        results.append(\"Skipped â€“ no examples\")\n",
    "        continue\n",
    "\n",
    "    votes = []\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 80,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            content = response.json()[\"response\"].strip()\n",
    "            label = extract_label(content)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error on row {i}: {e}\")\n",
    "            label = ON_FAIL_FALLBACK\n",
    "        votes.append(label)\n",
    "\n",
    "    final_label = Counter(votes).most_common(1)[0][0]\n",
    "    results.append(final_label)\n",
    "\n",
    "# === Save Initial Predictions ===\n",
    "mismatch_df = mismatch_df.iloc[start_idx:end_idx].copy()\n",
    "mismatch_df['llm_category'] = results\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"âœ… Self-consistency + few-shot results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "# === Reevaluate 3 vs 4 (plus 2) ===\n",
    "column_type_map = {\n",
    "    2: \"Sequencing type\",\n",
    "    3: \"Organism type\",\n",
    "    4: \"Tissue type\",\n",
    "    5: \"Cell line type\",\n",
    "    6: \"Disease type\"\n",
    "}\n",
    "\n",
    "def build_re_evaluation_prompt(row):\n",
    "    \"\"\"Focused re-prompt to distinguish 2 vs 3 vs 4\"\"\"\n",
    "    column_type = column_type_map.get(row[\"column_idx\"], \"unknown data type\"),\n",
    "    return f\"\"\"\n",
    "You are a domain-aware evaluator that classifies the relationship between a predicted answer and the actual (reference) answer from genomics studies data.\n",
    "\n",
    "- 2: Terminology Difference â€“ The predicted and actual terms refer to the same concept but use different terminology. These differences may include synonyms, abbreviations, or alternative phrasing that do not alter the underlying meaning. Use this only when the prediction and ground truth are equivalent in specificity and semantics.\n",
    "\n",
    "- 3: Generalization â€“ The predicted term is broader or less specific than the actual term. Some important contextual or descriptive detail from the ground truth is missing in the prediction. Use this when the prediction could apply to a wider range of concepts than intended.\n",
    "\n",
    "- 4: Too Specific â€“ The predicted term is more detailed or narrowly defined than the actual term. The prediction includes extra qualifiers, subtypes, or technical specifications that are not present in the actual. Use this when the prediction \"over-defines\" the intended label.\n",
    "\n",
    "Respond with just the number (2, 3, or 4).\n",
    "\n",
    "Example 1:\n",
    "Actual: lung adenocarcinoma\n",
    "Predicted: lung\n",
    "â†’ Answer: 3\n",
    "The predicted term is more general and omits the specific diagnosis.\n",
    "\n",
    "Example 2:\n",
    "Actual: lung\n",
    "Predicted: lung adenocarcinoma\n",
    "â†’ Answer: 4\n",
    "The prediction adds unnecessary detail that wasn't present in the actual.\n",
    "\n",
    "Example 3:\n",
    "Actual: ovarian carcinoma\n",
    "Predicted: ovarian cancer\n",
    "â†’ Answer: 2\n",
    "These terms are medically equivalent; one is slightly more technical.\n",
    "\n",
    "You are looking at a case with {column_type} data.\n",
    "\n",
    "Step 1: Compare the prediction and the actual answer. What are the differences?\n",
    "Step 2: Based on those differences, now evaluate:\n",
    "Actual: {row['truth_value']}\n",
    "Predicted: {row['llm_value']}\n",
    "â†’ Answer:\"\"\"\n",
    "\n",
    "print(\"\\nðŸ” Reevaluating rows with initial label 3 or 4...\\n\")\n",
    "\n",
    "refine_rows = mismatch_df[mismatch_df[\"llm_category\"].astype(str).isin([\"3\", \"4\"])]\n",
    "refined_labels = []\n",
    "\n",
    "for i, row in refine_rows.iterrows():\n",
    "    prompt = build_re_evaluation_prompt(row)\n",
    "    votes = []\n",
    "\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 80,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            content = response.json()[\"response\"].strip()\n",
    "            label = extract_label(content)\n",
    "            if label in {\"2\", \"3\", \"4\"}:\n",
    "                votes.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Reevaluation error on row {i}: {e}\")\n",
    "            votes.append(row[\"llm_category\"])  # fallback\n",
    "\n",
    "    final_label = Counter(votes).most_common(1)[0][0] if votes else row[\"llm_category\"]\n",
    "    refined_labels.append(final_label)\n",
    "\n",
    "mismatch_df.loc[refine_rows.index, \"llm_category\"] = refined_labels\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153dce0",
   "metadata": {},
   "source": [
    "Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4261db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Total Score (out of 736): 307.4\n",
      "Final Average Score: 0.418\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/Users/hannahyou/Documents/summer25roswell/important results/newest_round1.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "scoring = {\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 0.7,\n",
    "    4: 0.7,\n",
    "    5: 0.5,\n",
    "    6: 0.0\n",
    "}\n",
    "\n",
    "df['score'] = df['llm_category'].map(scoring)\n",
    "max_total = 736\n",
    "actual_predictions = len(df) - 1  #header row\n",
    "missing_rows = max_total - actual_predictions\n",
    "present_score = df['score'].sum()\n",
    "missing_score = missing_rows * 1.0\n",
    "final_total_score = present_score + missing_score\n",
    "final_average_score = final_total_score / max_total\n",
    "print(f\"Final Total Score (out of 736): {final_total_score}\")\n",
    "print(f\"Final Average Score: {round(final_average_score, 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
