{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7822887",
   "metadata": {},
   "source": [
    "fewshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8cae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cotbase_prompt_template = \"\"\"You are a domain-aware evaluator that classifies the relationship between a predicted answer and the actual (reference) answer.\n",
    "\n",
    "Use the following set of error categories to classify the mismatch:\n",
    "\n",
    "2. Terminology Difference – The predicted and actual answers refer to the exact same item using a different term\n",
    "3. Generalization – The predicted answer is more general than the actual answer.\n",
    "4. Too Specific – The predicted answer is more specific than the actual answer.\n",
    "5. Subset of Truth – The predicted answer is only part of the actual answer; doesnt include all of the actual answers. there must be multiple possible answers denoted by a \"/\" or a \";\" or \",\"\n",
    "6. Incorrect / Out of Scope / Hallucinated – The predicted answer is incorrect, unrelated, or introduces a false concept.\n",
    "\n",
    "Below are some examples for this task:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Step 1: Compare the prediction and the actual answer. What are the differences?\n",
    "Predicted: {predicted}\n",
    "Actual: {actual}\n",
    "\n",
    "Step 2: Based on those differences, classify the mismatch as type 2–6.\n",
    "\n",
    "Make sure nothing else is returned, only the number of the mismatch categor, which can be numbers 2, 3, 4, 5, or 6 (not 1). DOUBLE CHECK ITS ONLY THE NUMBER OF THE MISMATCH, only numbers should be returned, no words and no explainations. the answer should be formatted like this: \"2\" or \"3\" or \"4\" or \"5\" or \"6\" ONLY with no following text\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd3ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt_template = \"\"\"You are a domain-aware evaluator that classifies the relationship between a predicted answer and the actual (reference) answer from genomics studies data.\n",
    "\n",
    "\n",
    "Use these standardized mismatch labels:\n",
    "\n",
    "2. Terminology Difference – The predicted and actual answers refer to the exact same item using a different term\n",
    "3. Generalization – The predicted answer is more general than the actual answer.\n",
    "4. Too Specific – The predicted answer is more specific than the actual answer.\n",
    "5. Subset of Truth – The predicted answer is only part of the actual answer; doesnt include all of the actual answers. there must be multiple possible answers denoted by a \"/\" or a \";\" or \",\"\n",
    "6. Incorrect / Out of Scope / Hallucinated – The predicted answer is incorrect, unrelated, or introduces a false concept.\n",
    "\n",
    "Below are some examples for this task:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "You are looking at a case with {type_name} data.\n",
    "\n",
    "Step 1: Compare the prediction and the actual answer. What are the differences?\n",
    "Step 2: Based on those differences, classify the mismatch as type 2–6.\n",
    "\n",
    "Predicted: {predicted}\n",
    "Actual: {actual}\n",
    "\n",
    "\n",
    "Make sure nothing else is returned, only the number of the mismatch categor, which can be numbers 2, 3, 4, 5, or 6 (not 1). DOUBLE CHECK ITS ONLY THE NUMBER OF THE MISMATCH, only numbers should be returned, no words and no explainations. the answer should be formatted like this: \"2\" or \"3\" or \"4\" or \"5\" or \"6\" ONLY with no following text\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154869ba",
   "metadata": {},
   "source": [
    "\"\"\"You are a biomedical journal reviewer responsible for verifying that the predicted experimental labels in a submitted manuscript are appropriately aligned with the reference (gold standard) labels provided by domain experts.\n",
    "\n",
    "Use the following standardized mismatch categories to classify the relationship between the predicted and actual answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49130dbd",
   "metadata": {},
   "source": [
    "\"\"\"You are a clinical data auditor responsible for verifying the consistency between machine-generated predictions and human-validated labels in a biomedical dataset.\n",
    "\n",
    "Classify the mismatch between predicted and actual values using the following categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49823926",
   "metadata": {},
   "source": [
    "\"\"\"You are a biomedical nomenclature curator at a research institute. Your job is to evaluate whether predicted biomedical terms align semantically with standard reference terms.\n",
    "\n",
    "Use the following mismatch categories for classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc375f08",
   "metadata": {},
   "source": [
    " \"\"\"You are a Machine Learning dataset QA lead. Your responsibility is to verify whether predicted annotations align with the gold-standard annotations used to train biomedical models.\n",
    "\n",
    "Use these standardized mismatch labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fb1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_bank = {\n",
    "    '2': [\n",
    "        (\"Predicted: ovarian tumor\\nActual: ovarian cancer\", \"2: Terminology Difference – 'tumor' and 'cancer' are used interchangeably here, with no change in meaning.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: snrna-seq\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: rna-seq\\nActual: scrna-seq\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: scrna-seq; vdj-seq\", \"5: Subset of Truth – prediction does not include all details from truth, or is a component of truth.\"),\n",
    "        (\"Predicted: sctcr-seq\\nActual: scrna-seq; vdj-seq\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: merip-seq\\nActual: plumage\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: scrna-seq\\nActual: cite-seq\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "    ],\n",
    "    '4': [\n",
    "        (\"Predicted: ovarian carcinoma\\nActual: ovarian cancer\", \"2: Terminology Difference – different terms for the same disease.\"),\n",
    "        (\"Predicted: subq. tumor\\nActual: subcutaneous tumor\", \"2: Terminology Difference – abbreviation does not affect the meaning.\"),\n",
    "        (\"Predicted: cervical lymph node\\nActual: lymph node\", \"4: Too specific – prediction adds body region.\"),\n",
    "        (\"Predicted: colorectal carcinoma\\nActual: colon\", \"4: Too specific – prediction adds a cancer label.\"),\n",
    "        (\"Predicted: pancreatic neuroendocrine tumor\\nActual: pancreas\", \"4: Too specific – prediction adds a type of cancer.\"),\n",
    "        (\"Predicted: gastric wall\\nActual: stomach\", \"4: Too specific – prediction adds a body part detail.\"),\n",
    "        (\"Predicted: hepatocellular carcinoma\\nActual: liver\", \"4: Too specific – prediction adds a disease name.\"),\n",
    "        (\"Predicted: lung\\nActual: lung adenocarcinoma\", \"3: Generalization – predicted term is broader.\"),\n",
    "        (\"Predicted: liver\\nActual: hepatocellular carcinoma biopsy\", \"3: Generalization – predicted term is broader.\"),\n",
    "        (\"Predicted: bone marrow\\nActual: spleen\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: prostate cancer\\nActual: prostate\", \"4: Too specific – prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: prostate\\nActual: prostate carcinoma\", \"3: Generalization – predicted term is broader.\"),\n",
    "        (\"Predicted: breast tumor\\nActual: breast\", \"4: Too specific – prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: embryonic kidney\\nActual: kidney\", \"4: Too specific – prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: bone marrow\\nActual: blood\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "    ],\n",
    "    '5': [\n",
    "        (\"Predicted: lung squamous cell carcinoma cells\\nActual: lung squamous cell carcinoma\", \"2: Terminology Difference – adding 'cells' doesn't change the entity.\"),\n",
    "        (\"Predicted: aspc-1\\nActual: aspc1\", \"2: Terminology Difference – same cell line with a different naming style.\"),\n",
    "        (\"Predicted: rat lung adenocarcinoma cell line\\nActual: rat lung adenocarcinoma cells\", \"2: Terminology Difference – 'cell line' and 'cells' are interchangeable in this context.\"),\n",
    "        (\"Predicted: immortalized human dermal fibroblast line\\nActual: immortalized human dermal fibroblasts\", \"2: Terminology Difference – 'line' is included but doesn’t change the meaning.\"),\n",
    "        (\"Predicted: primary mouse mammary fibroblasts line\\nActual: primary mouse mammary fibroblasts\", \"2: Terminology Difference – 'line' is added, but it doesn’t meaningfully change the entity.\"),\n",
    "        (\"Predicted: kp lung tumor cells\\nActual: kp cells\", \"2: Terminology Difference – the added descriptors don’t change the identity of the cell line.\"),\n",
    "        (\"Predicted: gata3\\nActual: gata3-expressing cells\", \"3: Generalization – predicted term is broader.\"),\n",
    "        (\"Predicted: hek-ter\\nActual: hek cells\", \"4: Too specific – prediction has an extra detail not noted in the actual answer\"),\n",
    "        (\"Predicted: h1650\\nActual: h1651\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: hoxb8\\nActual: hoxb8 cell\", \"3: Generalization – predicted term is broader.\"),\n",
    "        (\"Predicted: no\\nActual: hoxb8 cell\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: d458\\nActual: d459\", \"6: Incorrect / Out of Scope / Hallucinated – completely different or unclassifiable.\"),\n",
    "        (\"Predicted: myc-cap\\nActual: myc-cap cells\", \"3: Generalization – predicted term is broader.\"),\n",
    "    ],\n",
    "    '3': [\n",
    "        (\"Predicted: mus musculus\\nActual: mus musculus; synthetic construct\", \"5: Subset of Truth – prediction does not include all details from truth, or is a component of truth.\"),\n",
    "        (\"Predicted: homo sapiens\\nActual: synthetic construct; homo sapiens and murine mouse model\", \"3: Generalization – predicted term is broader.\"),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d60876",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_type_map = {\n",
    "    2: \"Sequencing type\",\n",
    "    3: \"Organism type\",\n",
    "    4: \"Tissue type\",\n",
    "    5: \"Cell line type\",\n",
    "    6: \"Disease type\"\n",
    "}\n",
    "\n",
    "def build_prompt(row):\n",
    "    task = str(row[1])      # Column 2 is the numeric task as string\n",
    "    predicted = str(row[2])              # Column 3 is predicted\n",
    "    actual = str(row[3])                 # Column 4 is actual\n",
    "    column_idx = int(row['column_idx'])\n",
    "\n",
    "    type_name = column_type_map.get(column_idx, \"feature\")\n",
    "\n",
    "    # Format examples\n",
    "    examples = few_shot_bank[task]\n",
    "    few_shot_str = \"\\n\\n\".join([\n",
    "        f\"{ex[0]}\\n{ex[1]}\" for ex in examples\n",
    "    ])\n",
    "\n",
    "    # Return the final prompt\n",
    "    return base_prompt_template.format(\n",
    "        few_shot_examples=few_shot_str,\n",
    "        predicted=predicted,\n",
    "        actual=actual,\n",
    "        type_name=type_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf343871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  task = str(row[1])      # Column 2 is the numeric task as string\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  predicted = str(row[2])              # Column 3 is predicted\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  actual = str(row[3])                 # Column 4 is actual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to: /Users/yjyou/Documents/summer25/result_role_machinelearner.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load your CSV (double check case-sensitive path!)\n",
    "mismatch_df = pd.read_csv(\"/Users/yjyou/Documents/summer25/updated_testset175_mwe3.csv\")\n",
    "\n",
    "results = []\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = 176  # Update if your dataset changes\n",
    "\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "for i, row in mismatch_df.iloc[start_idx:end_idx].iterrows():\n",
    "    prompt = build_prompt(row)  # Make sure build_prompt() is defined above this\n",
    "    if prompt is None:\n",
    "        results.append(\"Skipped – no examples\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\n",
    "                \"model\": \"llama3\",  # Replace with another model like \"phi3\" if needed\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        content = response.json()[\"response\"].strip()\n",
    "        results.append(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {i}: {e}\")\n",
    "        results.append(f\"Error: {str(e)}\")\n",
    "\n",
    "# Save the results to your local folder\n",
    "mismatch_df = mismatch_df.iloc[start_idx:end_idx].copy()  # Ensure only the processed rows are included\n",
    "mismatch_df['llm_category'] = results\n",
    "output_df = mismatch_df\n",
    "\n",
    "output_path = \"/Users/yjyou/Documents/summer25/result_role_machinelearner.csv\"\n",
    "output_df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db686fd",
   "metadata": {},
   "source": [
    "self-consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1911e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/2594603563.py:65: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  predicted = str(row[2])\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/2594603563.py:66: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  actual = str(row[3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean self-consistency results saved to: /Users/yjyou/Documents/summer25/result_SC_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# === Config ===\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3\"\n",
    "INPUT_CSV = \"/Users/yjyou/Documents/summer25/updated_testset175_mwe3.csv\"\n",
    "OUTPUT_CSV = \"/Users/yjyou/Documents/summer25/result_SC_clean.csv\"\n",
    "SC_SAMPLES = 5\n",
    "SC_TEMPERATURE = 0.85\n",
    "ON_FAIL_FALLBACK = \"6\"\n",
    "digit_re = re.compile(r\"\\b([2-6])\\b\")\n",
    "\n",
    "# === Helper functions ===\n",
    "def extract_label(text, fallback=ON_FAIL_FALLBACK):\n",
    "    if not isinstance(text, str): return fallback\n",
    "    m = digit_re.search(text.strip())\n",
    "    return m.group(1) if m else fallback\n",
    "\n",
    "def call_ollama(prompt, temperature=0.8, seed=None, timeout=60):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "    }\n",
    "    if seed is not None:\n",
    "        payload[\"options\"][\"seed\"] = seed\n",
    "    resp = requests.post(OLLAMA_API_URL, json=payload, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"response\"].strip()\n",
    "\n",
    "# === Prompt template (no few-shot) ===\n",
    "def build_clean_prompt(predicted, actual):\n",
    "    return f\"\"\"\n",
    "You are a careful classification model. Your job is to determine the type of mismatch between a predicted label and an actual label in biomedical data.\n",
    "\n",
    "The mismatch type must be one of the following numbers:\n",
    "2 = Terminology difference\n",
    "3 = Generalization\n",
    "4 = Too specific\n",
    "5 = Subset of truth\n",
    "6 = Completely incorrect / unrelated\n",
    "\n",
    "Given the following pair:\n",
    "Predicted: {predicted}\n",
    "Actual: {actual}\n",
    "\n",
    "What is the most likely mismatch type? Return ONLY the number (2, 3, 4, 5, or 6).\n",
    "\"\"\"\n",
    "\n",
    "# === Main runner ===\n",
    "def run_self_consistency_clean():\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    preds = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        predicted = str(row[2])\n",
    "        actual = str(row[3])\n",
    "        prompt = build_clean_prompt(predicted, actual)\n",
    "        votes = []\n",
    "\n",
    "        for _ in range(SC_SAMPLES):\n",
    "            try:\n",
    "                seed = random.randint(1, 1_000_000)\n",
    "                out = call_ollama(prompt, temperature=SC_TEMPERATURE, seed=seed)\n",
    "                label = extract_label(out)\n",
    "            except:\n",
    "                label = ON_FAIL_FALLBACK\n",
    "            votes.append(label)\n",
    "\n",
    "        final = Counter(votes).most_common(1)[0][0]\n",
    "        preds.append(final)\n",
    "\n",
    "    df[\"llm_category\"] = preds\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Clean self-consistency results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "\n",
    "run_self_consistency_clean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0f9b7",
   "metadata": {},
   "source": [
    "fewshot and self consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd949c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing rows 0 to 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_77319/3227016779.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  task = str(row[1])      # Column 2 is the numeric task as string\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_77319/3227016779.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  predicted = str(row[2])              # Column 3 is predicted\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_77319/3227016779.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  actual = str(row[3])                 # Column 4 is actual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-consistency + few-shot results saved to: /Users/yjyou/Documents/summer25/result_SC_fewshot_2step1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Config ===\n",
    "INPUT_CSV = \"/Users/yjyou/Documents/summer25/updated_testset175_mwe3.csv\"\n",
    "OUTPUT_CSV = \"/Users/yjyou/Documents/summer25/result_SC_fewshot_2step1.csv\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"gemma3:12b\"\n",
    "SC_SAMPLES = 5\n",
    "SC_TEMPERATURE = 0.90\n",
    "ON_FAIL_FALLBACK = \"6\"\n",
    "\n",
    "# === Label Extractor ===\n",
    "digit_re = re.compile(r\"\\b([2-6])\\b\")\n",
    "def extract_label(text):\n",
    "    if not isinstance(text, str): return ON_FAIL_FALLBACK\n",
    "    match = digit_re.search(text.strip())\n",
    "    return match.group(1) if match else ON_FAIL_FALLBACK\n",
    "\n",
    "# === Load CSV ===\n",
    "mismatch_df = pd.read_csv(INPUT_CSV)\n",
    "results = []\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = len(mismatch_df)  # or 176 if you're limiting\n",
    "print(f\"🔍 Processing rows {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "# === Run Self-Consistency + Few-Shot ===\n",
    "for i, row in mismatch_df.iloc[start_idx:end_idx].iterrows():\n",
    "    prompt = build_prompt(row)  # Assumes you defined this already\n",
    "    if prompt is None:\n",
    "        results.append(\"Skipped – no examples\")\n",
    "        continue\n",
    "\n",
    "    votes = []\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 80,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            content = response.json()[\"response\"].strip()\n",
    "            label = extract_label(content)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on row {i}: {e}\")\n",
    "            label = ON_FAIL_FALLBACK\n",
    "        votes.append(label)\n",
    "\n",
    "    final_label = Counter(votes).most_common(1)[0][0]\n",
    "    results.append(final_label)\n",
    "\n",
    "# === Save to CSV ===\n",
    "mismatch_df = mismatch_df.iloc[start_idx:end_idx].copy()\n",
    "mismatch_df['llm_category'] = results\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Self-consistency + few-shot results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37638440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing rows 0 to 469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_31280/3227016779.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  task = str(row[1])      # Column 2 is the numeric task as string\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_31280/3227016779.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  predicted = str(row[2])              # Column 3 is predicted\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_31280/3227016779.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  actual = str(row[3])                 # Column 4 is actual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-consistency + few-shot results saved to: /Users/yjyou/Documents/summer25/newest_round3.csv\n",
      "\n",
      "🔁 Reevaluating rows with initial label 3 or 4...\n",
      "\n",
      "✅ Final reevaluated results saved to: /Users/yjyou/Documents/summer25/newest_round3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Config ===\n",
    "INPUT_CSV = \"/Users/yjyou/Documents/summer25/updated_mismatch_with_errors3.csv\"\n",
    "OUTPUT_CSV = \"/Users/yjyou/Documents/summer25/newest_round3.csv\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"gemma3:27b\"\n",
    "SC_SAMPLES = 5\n",
    "SC_TEMPERATURE = 0.95\n",
    "ON_FAIL_FALLBACK = \"6\"\n",
    "\n",
    "# === Label Extractor ===\n",
    "digit_re = re.compile(r\"\\b([2-6])\\b\")\n",
    "def extract_label(text):\n",
    "    if not isinstance(text, str): return ON_FAIL_FALLBACK\n",
    "    match = digit_re.search(text.strip())\n",
    "    return match.group(1) if match else ON_FAIL_FALLBACK\n",
    "\n",
    "# === Load CSV ===\n",
    "mismatch_df = pd.read_csv(INPUT_CSV)\n",
    "results = []\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = len(mismatch_df)  # or 176 if you're limiting\n",
    "print(f\"🔍 Processing rows {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "# === Run Self-Consistency + Few-Shot ===\n",
    "for i, row in mismatch_df.iloc[start_idx:end_idx].iterrows():\n",
    "    prompt = build_prompt(row)  # Assumes you defined this already\n",
    "    if prompt is None:\n",
    "        results.append(\"Skipped – no examples\")\n",
    "        continue\n",
    "\n",
    "    votes = []\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 80,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            content = response.json()[\"response\"].strip()\n",
    "            label = extract_label(content)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on row {i}: {e}\")\n",
    "            label = ON_FAIL_FALLBACK\n",
    "        votes.append(label)\n",
    "\n",
    "    final_label = Counter(votes).most_common(1)[0][0]\n",
    "    results.append(final_label)\n",
    "\n",
    "# === Save Initial Predictions ===\n",
    "mismatch_df = mismatch_df.iloc[start_idx:end_idx].copy()\n",
    "mismatch_df['llm_category'] = results\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Self-consistency + few-shot results saved to: {OUTPUT_CSV}\")\n",
    "\n",
    "# === Reevaluate 3 vs 4 (plus 2) ===\n",
    "\n",
    "column_type_map = {\n",
    "    2: \"Sequencing type\",\n",
    "    3: \"Organism type\",\n",
    "    4: \"Tissue type\",\n",
    "    5: \"Cell line type\",\n",
    "    6: \"Disease type\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_re_evaluation_prompt(row):\n",
    "    \"\"\"Focused re-prompt to distinguish 2 vs 3 vs 4\"\"\"\n",
    "    column_type = column_type_map.get(row[\"column_idx\"], \"unknown data type\"),\n",
    "    return f\"\"\"\n",
    "You are a domain-aware evaluator that classifies the relationship between a predicted answer and the actual (reference) answer from genomics studies data.\n",
    "\n",
    "- 2: Terminology Difference – The predicted and actual terms refer to the same concept but use different terminology. These differences may include synonyms, abbreviations, or alternative phrasing that do not alter the underlying meaning. Use this only when the prediction and ground truth are equivalent in specificity and semantics.\n",
    "\n",
    "- 3: Generalization – The predicted term is broader or less specific than the actual term. Some important contextual or descriptive detail from the ground truth is missing in the prediction. Use this when the prediction could apply to a wider range of concepts than intended.\n",
    "\n",
    "- 4: Too Specific – The predicted term is more detailed or narrowly defined than the actual term. The prediction includes extra qualifiers, subtypes, or technical specifications that are not present in the actual. Use this when the prediction \"over-defines\" the intended label.\n",
    "\n",
    "Respond with just the number (2, 3, or 4).\n",
    "\n",
    "Example 1:\n",
    "Actual: lung adenocarcinoma\n",
    "Predicted: lung\n",
    "→ Answer: 3\n",
    "The predicted term is more general and omits the specific diagnosis.\n",
    "\n",
    "Example 2:\n",
    "Actual: lung\n",
    "Predicted: lung adenocarcinoma\n",
    "→ Answer: 4\n",
    "The prediction adds unnecessary detail that wasn't present in the actual.\n",
    "\n",
    "Example 3:\n",
    "Actual: ovarian carcinoma\n",
    "Predicted: ovarian cancer\n",
    "→ Answer: 2\n",
    "These terms are medically equivalent; one is slightly more technical.\n",
    "\n",
    "You are looking at a case with {column_type} data.\n",
    "\n",
    "Step 1: Compare the prediction and the actual answer. What are the differences?\n",
    "Step 2: Based on those differences, now evaluate:\n",
    "Actual: {row['truth_value']}\n",
    "Predicted: {row['llm_value']}\n",
    "→ Answer:\"\"\"\n",
    "\n",
    "print(\"\\n🔁 Reevaluating rows with initial label 3 or 4...\\n\")\n",
    "\n",
    "refine_rows = mismatch_df[mismatch_df[\"llm_category\"].astype(str).isin([\"3\", \"4\"])]\n",
    "refined_labels = []\n",
    "\n",
    "for i, row in refine_rows.iterrows():\n",
    "    prompt = build_re_evaluation_prompt(row)\n",
    "    votes = []\n",
    "\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 80,\n",
    "                        \"top_p\": 1,\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            content = response.json()[\"response\"].strip()\n",
    "            label = extract_label(content)\n",
    "            if label in {\"2\", \"3\", \"4\"}:\n",
    "                votes.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Reevaluation error on row {i}: {e}\")\n",
    "            votes.append(row[\"llm_category\"])  # fallback\n",
    "\n",
    "    final_label = Counter(votes).most_common(1)[0][0] if votes else row[\"llm_category\"]\n",
    "    refined_labels.append(final_label)\n",
    "\n",
    "# Apply refined labels\n",
    "mismatch_df.loc[refine_rows.index, \"llm_category\"] = refined_labels\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Final reevaluated results saved to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c43ea542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing rows 0 to 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  task = str(row[1])      # Column 2 is the numeric task as string\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  predicted = str(row[2])              # Column 3 is predicted\n",
      "/var/folders/6l/8_9wf1r551s6b1f3mmsxpn_mtd197j/T/ipykernel_58832/3312944127.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  actual = str(row[3])                 # Column 4 is actual\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Self-consistency + few-shot results saved to: /Users/yjyou/Documents/summer25/result_SC_debate_fewshot.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# === Config ===\n",
    "INPUT_CSV = \"/Users/yjyou/Documents/summer25/updated_testset175_mwe3.csv\"\n",
    "OUTPUT_CSV = \"/Users/yjyou/Documents/summer25/result_SC_debate_fewshot.csv\"\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"llama3\"\n",
    "SC_SAMPLES = 5\n",
    "SC_TEMPERATURE = 1.0\n",
    "ON_FAIL_FALLBACK = \"6\"\n",
    "\n",
    "# === Label Extractor ===\n",
    "digit_re = re.compile(r\"\\b([2-6])\\b\")\n",
    "def extract_label(text):\n",
    "    if not isinstance(text, str): return ON_FAIL_FALLBACK\n",
    "    match = digit_re.search(text.strip())\n",
    "    return match.group(1) if match else ON_FAIL_FALLBACK\n",
    "\n",
    "# === Load CSV ===\n",
    "mismatch_df = pd.read_csv(INPUT_CSV)\n",
    "results = []\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = len(mismatch_df)  # or 176 if you're limiting\n",
    "print(f\"🔍 Processing rows {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "# === Run Self-Consistency + Few-Shot ===\n",
    "for i, row in mismatch_df.iloc[start_idx:end_idx].iterrows():\n",
    "    prompt = build_prompt(row)\n",
    "    if prompt is None:\n",
    "        results.append(\"Skipped – no examples\")\n",
    "        continue\n",
    "\n",
    "    votes = []\n",
    "    raw_responses = []\n",
    "\n",
    "    for _ in range(SC_SAMPLES):\n",
    "        try:\n",
    "            seed = random.randint(1, 1_000_000)\n",
    "            response = requests.post(\n",
    "                OLLAMA_API_URL,\n",
    "                json={\n",
    "                    \"model\": MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": SC_TEMPERATURE,\n",
    "                        \"seed\": seed,\n",
    "                        \"top_k\": 40,\n",
    "                        \"top_p\": 0.9\n",
    "                    }\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            raw = response.json()[\"response\"].strip()\n",
    "            raw_responses.append(raw)\n",
    "            label = extract_label(raw)\n",
    "            votes.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error on row {i}: {e}\")\n",
    "            votes.append(ON_FAIL_FALLBACK)\n",
    "            raw_responses.append(\"Error\")\n",
    "\n",
    "    # 🧠 Debate Prompt\n",
    "    debate_prompt = f\"\"\"You answered this classification question five times and gave the following responses:\n",
    "\n",
    "{chr(10).join([f\"{j+1}. {resp}\" for j, resp in enumerate(raw_responses)])}\n",
    "\n",
    "Based on these responses, which one seems the most accurate? Choose from the options: 2, 3, 4, 5, 6.\n",
    "Return ONLY the number of the best answer. No explanation.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        debate_response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"prompt\": debate_prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"seed\": random.randint(1, 1_000_000),\n",
    "                    \"top_k\": 40,\n",
    "                    \"top_p\": 0.9\n",
    "                }\n",
    "            },\n",
    "            timeout=60\n",
    "        )\n",
    "        debate_response.raise_for_status()\n",
    "        debate_label = extract_label(debate_response.json()[\"response\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Debate error on row {i}: {e}\")\n",
    "        debate_label = Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "    results.append(debate_label)\n",
    "\n",
    "\n",
    "\n",
    "# === Save to CSV ===\n",
    "mismatch_df = mismatch_df.iloc[start_idx:end_idx].copy()\n",
    "mismatch_df['llm_category'] = results\n",
    "mismatch_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"✅ Self-consistency + few-shot results saved to: {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
